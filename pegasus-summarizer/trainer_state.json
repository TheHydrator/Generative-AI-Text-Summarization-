{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9997284822155851,
  "eval_steps": 500,
  "global_step": 1841,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005430355688297583,
      "grad_norm": 16.521413803100586,
      "learning_rate": 7.000000000000001e-07,
      "loss": 2.9962,
      "step": 10
    },
    {
      "epoch": 0.010860711376595167,
      "grad_norm": 19.629636764526367,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 3.4264,
      "step": 20
    },
    {
      "epoch": 0.01629106706489275,
      "grad_norm": 15.37358283996582,
      "learning_rate": 2.7e-06,
      "loss": 2.9134,
      "step": 30
    },
    {
      "epoch": 0.021721422753190334,
      "grad_norm": 13.720174789428711,
      "learning_rate": 3.6e-06,
      "loss": 2.9706,
      "step": 40
    },
    {
      "epoch": 0.027151778441487917,
      "grad_norm": 26.965559005737305,
      "learning_rate": 4.6e-06,
      "loss": 2.96,
      "step": 50
    },
    {
      "epoch": 0.0325821341297855,
      "grad_norm": 13.172840118408203,
      "learning_rate": 5.500000000000001e-06,
      "loss": 2.9524,
      "step": 60
    },
    {
      "epoch": 0.038012489818083084,
      "grad_norm": 12.033304214477539,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 2.7016,
      "step": 70
    },
    {
      "epoch": 0.04344284550638067,
      "grad_norm": 10.598291397094727,
      "learning_rate": 7.4e-06,
      "loss": 2.8206,
      "step": 80
    },
    {
      "epoch": 0.04887320119467825,
      "grad_norm": 11.378610610961914,
      "learning_rate": 8.400000000000001e-06,
      "loss": 2.4319,
      "step": 90
    },
    {
      "epoch": 0.054303556882975834,
      "grad_norm": 20.38823699951172,
      "learning_rate": 9.4e-06,
      "loss": 2.3078,
      "step": 100
    },
    {
      "epoch": 0.05973391257127342,
      "grad_norm": 61.104557037353516,
      "learning_rate": 1.04e-05,
      "loss": 2.4044,
      "step": 110
    },
    {
      "epoch": 0.065164268259571,
      "grad_norm": 16.90481185913086,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 2.2971,
      "step": 120
    },
    {
      "epoch": 0.07059462394786858,
      "grad_norm": 66.30975341796875,
      "learning_rate": 1.24e-05,
      "loss": 2.4719,
      "step": 130
    },
    {
      "epoch": 0.07602497963616617,
      "grad_norm": 9.893631935119629,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 2.0954,
      "step": 140
    },
    {
      "epoch": 0.08145533532446375,
      "grad_norm": 8.793401718139648,
      "learning_rate": 1.44e-05,
      "loss": 2.245,
      "step": 150
    },
    {
      "epoch": 0.08688569101276133,
      "grad_norm": 18.801799774169922,
      "learning_rate": 1.54e-05,
      "loss": 2.024,
      "step": 160
    },
    {
      "epoch": 0.09231604670105892,
      "grad_norm": 55.205848693847656,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 2.0256,
      "step": 170
    },
    {
      "epoch": 0.0977464023893565,
      "grad_norm": 37.59559631347656,
      "learning_rate": 1.74e-05,
      "loss": 2.1034,
      "step": 180
    },
    {
      "epoch": 0.10317675807765408,
      "grad_norm": 5.863060474395752,
      "learning_rate": 1.84e-05,
      "loss": 2.1899,
      "step": 190
    },
    {
      "epoch": 0.10860711376595167,
      "grad_norm": 7.9860029220581055,
      "learning_rate": 1.94e-05,
      "loss": 2.0729,
      "step": 200
    },
    {
      "epoch": 0.11403746945424925,
      "grad_norm": 7.346916198730469,
      "learning_rate": 2.04e-05,
      "loss": 2.0028,
      "step": 210
    },
    {
      "epoch": 0.11946782514254684,
      "grad_norm": 9.388072967529297,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 1.8323,
      "step": 220
    },
    {
      "epoch": 0.12489818083084442,
      "grad_norm": 7.3489251136779785,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 2.0544,
      "step": 230
    },
    {
      "epoch": 0.130328536519142,
      "grad_norm": 21.767784118652344,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 1.9166,
      "step": 240
    },
    {
      "epoch": 0.13575889220743959,
      "grad_norm": 7.279740333557129,
      "learning_rate": 2.44e-05,
      "loss": 1.8342,
      "step": 250
    },
    {
      "epoch": 0.14118924789573717,
      "grad_norm": 8.65565013885498,
      "learning_rate": 2.54e-05,
      "loss": 1.8358,
      "step": 260
    },
    {
      "epoch": 0.14661960358403475,
      "grad_norm": 7.065512657165527,
      "learning_rate": 2.64e-05,
      "loss": 1.8648,
      "step": 270
    },
    {
      "epoch": 0.15204995927233234,
      "grad_norm": 6.313521385192871,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 1.9136,
      "step": 280
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 5.070932865142822,
      "learning_rate": 2.84e-05,
      "loss": 1.8695,
      "step": 290
    },
    {
      "epoch": 0.1629106706489275,
      "grad_norm": 6.004004955291748,
      "learning_rate": 2.94e-05,
      "loss": 1.8595,
      "step": 300
    },
    {
      "epoch": 0.16834102633722509,
      "grad_norm": 7.8575310707092285,
      "learning_rate": 3.04e-05,
      "loss": 1.7497,
      "step": 310
    },
    {
      "epoch": 0.17377138202552267,
      "grad_norm": 7.060926914215088,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 1.7784,
      "step": 320
    },
    {
      "epoch": 0.17920173771382025,
      "grad_norm": 9.498136520385742,
      "learning_rate": 3.24e-05,
      "loss": 1.7813,
      "step": 330
    },
    {
      "epoch": 0.18463209340211784,
      "grad_norm": 6.212674140930176,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 1.8308,
      "step": 340
    },
    {
      "epoch": 0.19006244909041542,
      "grad_norm": 11.363799095153809,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 1.7409,
      "step": 350
    },
    {
      "epoch": 0.195492804778713,
      "grad_norm": 11.097092628479004,
      "learning_rate": 3.54e-05,
      "loss": 1.7611,
      "step": 360
    },
    {
      "epoch": 0.2009231604670106,
      "grad_norm": 5.175985813140869,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 1.6074,
      "step": 370
    },
    {
      "epoch": 0.20635351615530817,
      "grad_norm": 5.770750999450684,
      "learning_rate": 3.74e-05,
      "loss": 1.776,
      "step": 380
    },
    {
      "epoch": 0.21178387184360575,
      "grad_norm": 4.407259941101074,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 1.7511,
      "step": 390
    },
    {
      "epoch": 0.21721422753190334,
      "grad_norm": 5.1034345626831055,
      "learning_rate": 3.94e-05,
      "loss": 1.7085,
      "step": 400
    },
    {
      "epoch": 0.22264458322020092,
      "grad_norm": 7.0920233726501465,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 1.7566,
      "step": 410
    },
    {
      "epoch": 0.2280749389084985,
      "grad_norm": 5.1377763748168945,
      "learning_rate": 4.14e-05,
      "loss": 1.6617,
      "step": 420
    },
    {
      "epoch": 0.2335052945967961,
      "grad_norm": 5.051285266876221,
      "learning_rate": 4.24e-05,
      "loss": 1.6191,
      "step": 430
    },
    {
      "epoch": 0.23893565028509367,
      "grad_norm": 5.431254863739014,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 1.6362,
      "step": 440
    },
    {
      "epoch": 0.24436600597339125,
      "grad_norm": 6.811744213104248,
      "learning_rate": 4.44e-05,
      "loss": 1.6619,
      "step": 450
    },
    {
      "epoch": 0.24979636166168884,
      "grad_norm": 5.617849826812744,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 1.7664,
      "step": 460
    },
    {
      "epoch": 0.2552267173499864,
      "grad_norm": 6.7975687980651855,
      "learning_rate": 4.64e-05,
      "loss": 1.6846,
      "step": 470
    },
    {
      "epoch": 0.260657073038284,
      "grad_norm": 4.585092067718506,
      "learning_rate": 4.74e-05,
      "loss": 1.6301,
      "step": 480
    },
    {
      "epoch": 0.2660874287265816,
      "grad_norm": 5.503396987915039,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 1.6408,
      "step": 490
    },
    {
      "epoch": 0.27151778441487917,
      "grad_norm": 8.601105690002441,
      "learning_rate": 4.94e-05,
      "loss": 1.7048,
      "step": 500
    },
    {
      "epoch": 0.27694814010317675,
      "grad_norm": 5.784457683563232,
      "learning_rate": 4.985085756897837e-05,
      "loss": 1.6964,
      "step": 510
    },
    {
      "epoch": 0.28237849579147434,
      "grad_norm": 5.613561630249023,
      "learning_rate": 4.947800149142432e-05,
      "loss": 1.7121,
      "step": 520
    },
    {
      "epoch": 0.2878088514797719,
      "grad_norm": 8.032981872558594,
      "learning_rate": 4.910514541387025e-05,
      "loss": 1.6389,
      "step": 530
    },
    {
      "epoch": 0.2932392071680695,
      "grad_norm": 8.015218734741211,
      "learning_rate": 4.8732289336316184e-05,
      "loss": 1.5957,
      "step": 540
    },
    {
      "epoch": 0.2986695628563671,
      "grad_norm": 4.551911354064941,
      "learning_rate": 4.835943325876212e-05,
      "loss": 1.6135,
      "step": 550
    },
    {
      "epoch": 0.30409991854466467,
      "grad_norm": 4.590435981750488,
      "learning_rate": 4.798657718120805e-05,
      "loss": 1.7001,
      "step": 560
    },
    {
      "epoch": 0.30953027423296225,
      "grad_norm": 13.446117401123047,
      "learning_rate": 4.76510067114094e-05,
      "loss": 1.7668,
      "step": 570
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 7.04325532913208,
      "learning_rate": 4.7278150633855335e-05,
      "loss": 1.6861,
      "step": 580
    },
    {
      "epoch": 0.3203909856095574,
      "grad_norm": 4.819209575653076,
      "learning_rate": 4.690529455630127e-05,
      "loss": 1.6289,
      "step": 590
    },
    {
      "epoch": 0.325821341297855,
      "grad_norm": 3.7041633129119873,
      "learning_rate": 4.65324384787472e-05,
      "loss": 1.5773,
      "step": 600
    },
    {
      "epoch": 0.3312516969861526,
      "grad_norm": 21.274322509765625,
      "learning_rate": 4.615958240119314e-05,
      "loss": 1.7599,
      "step": 610
    },
    {
      "epoch": 0.33668205267445017,
      "grad_norm": 6.363802909851074,
      "learning_rate": 4.5786726323639076e-05,
      "loss": 1.7384,
      "step": 620
    },
    {
      "epoch": 0.34211240836274776,
      "grad_norm": 4.45992374420166,
      "learning_rate": 4.541387024608501e-05,
      "loss": 1.9065,
      "step": 630
    },
    {
      "epoch": 0.34754276405104534,
      "grad_norm": 5.586881637573242,
      "learning_rate": 4.504101416853095e-05,
      "loss": 1.6926,
      "step": 640
    },
    {
      "epoch": 0.3529731197393429,
      "grad_norm": 4.485104084014893,
      "learning_rate": 4.466815809097689e-05,
      "loss": 1.7943,
      "step": 650
    },
    {
      "epoch": 0.3584034754276405,
      "grad_norm": 6.863260269165039,
      "learning_rate": 4.4295302013422824e-05,
      "loss": 1.683,
      "step": 660
    },
    {
      "epoch": 0.3638338311159381,
      "grad_norm": 4.151666164398193,
      "learning_rate": 4.3922445935868754e-05,
      "loss": 1.6123,
      "step": 670
    },
    {
      "epoch": 0.3692641868042357,
      "grad_norm": 4.4419755935668945,
      "learning_rate": 4.354958985831469e-05,
      "loss": 1.6449,
      "step": 680
    },
    {
      "epoch": 0.37469454249253326,
      "grad_norm": 4.350715637207031,
      "learning_rate": 4.317673378076063e-05,
      "loss": 1.6686,
      "step": 690
    },
    {
      "epoch": 0.38012489818083084,
      "grad_norm": 4.792738914489746,
      "learning_rate": 4.2803877703206565e-05,
      "loss": 1.6354,
      "step": 700
    },
    {
      "epoch": 0.3855552538691284,
      "grad_norm": 5.516550064086914,
      "learning_rate": 4.24310216256525e-05,
      "loss": 1.4811,
      "step": 710
    },
    {
      "epoch": 0.390985609557426,
      "grad_norm": 4.468837261199951,
      "learning_rate": 4.205816554809844e-05,
      "loss": 1.6467,
      "step": 720
    },
    {
      "epoch": 0.3964159652457236,
      "grad_norm": 4.777517318725586,
      "learning_rate": 4.168530947054437e-05,
      "loss": 1.6214,
      "step": 730
    },
    {
      "epoch": 0.4018463209340212,
      "grad_norm": 5.646853923797607,
      "learning_rate": 4.131245339299031e-05,
      "loss": 1.5672,
      "step": 740
    },
    {
      "epoch": 0.40727667662231876,
      "grad_norm": 5.795638561248779,
      "learning_rate": 4.0939597315436244e-05,
      "loss": 1.52,
      "step": 750
    },
    {
      "epoch": 0.41270703231061634,
      "grad_norm": 5.685527801513672,
      "learning_rate": 4.056674123788218e-05,
      "loss": 1.698,
      "step": 760
    },
    {
      "epoch": 0.4181373879989139,
      "grad_norm": 5.806288719177246,
      "learning_rate": 4.019388516032812e-05,
      "loss": 1.7035,
      "step": 770
    },
    {
      "epoch": 0.4235677436872115,
      "grad_norm": 5.0050225257873535,
      "learning_rate": 3.9821029082774055e-05,
      "loss": 1.5351,
      "step": 780
    },
    {
      "epoch": 0.4289980993755091,
      "grad_norm": 4.163165092468262,
      "learning_rate": 3.9448173005219985e-05,
      "loss": 1.6577,
      "step": 790
    },
    {
      "epoch": 0.4344284550638067,
      "grad_norm": 15.194209098815918,
      "learning_rate": 3.907531692766592e-05,
      "loss": 1.6117,
      "step": 800
    },
    {
      "epoch": 0.43985881075210426,
      "grad_norm": 6.819314479827881,
      "learning_rate": 3.870246085011186e-05,
      "loss": 1.5892,
      "step": 810
    },
    {
      "epoch": 0.44528916644040184,
      "grad_norm": 4.940896511077881,
      "learning_rate": 3.832960477255779e-05,
      "loss": 1.6949,
      "step": 820
    },
    {
      "epoch": 0.4507195221286994,
      "grad_norm": 8.392663955688477,
      "learning_rate": 3.7956748695003727e-05,
      "loss": 1.5495,
      "step": 830
    },
    {
      "epoch": 0.456149877816997,
      "grad_norm": 7.14751672744751,
      "learning_rate": 3.758389261744967e-05,
      "loss": 1.5451,
      "step": 840
    },
    {
      "epoch": 0.4615802335052946,
      "grad_norm": 3.36195707321167,
      "learning_rate": 3.72110365398956e-05,
      "loss": 1.6619,
      "step": 850
    },
    {
      "epoch": 0.4670105891935922,
      "grad_norm": 23.522846221923828,
      "learning_rate": 3.683818046234154e-05,
      "loss": 1.7877,
      "step": 860
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 4.125504970550537,
      "learning_rate": 3.6465324384787475e-05,
      "loss": 1.6214,
      "step": 870
    },
    {
      "epoch": 0.47787130057018734,
      "grad_norm": 8.224037170410156,
      "learning_rate": 3.6092468307233405e-05,
      "loss": 1.6453,
      "step": 880
    },
    {
      "epoch": 0.4833016562584849,
      "grad_norm": 4.352176189422607,
      "learning_rate": 3.571961222967934e-05,
      "loss": 1.6066,
      "step": 890
    },
    {
      "epoch": 0.4887320119467825,
      "grad_norm": 11.778558731079102,
      "learning_rate": 3.534675615212528e-05,
      "loss": 1.5413,
      "step": 900
    },
    {
      "epoch": 0.4941623676350801,
      "grad_norm": 5.509754180908203,
      "learning_rate": 3.497390007457122e-05,
      "loss": 1.651,
      "step": 910
    },
    {
      "epoch": 0.4995927233233777,
      "grad_norm": 6.074796199798584,
      "learning_rate": 3.460104399701715e-05,
      "loss": 1.619,
      "step": 920
    },
    {
      "epoch": 0.5050230790116753,
      "grad_norm": 9.978692054748535,
      "learning_rate": 3.422818791946309e-05,
      "loss": 1.5216,
      "step": 930
    },
    {
      "epoch": 0.5104534346999728,
      "grad_norm": 4.7971882820129395,
      "learning_rate": 3.385533184190903e-05,
      "loss": 1.7115,
      "step": 940
    },
    {
      "epoch": 0.5158837903882705,
      "grad_norm": 5.1472296714782715,
      "learning_rate": 3.348247576435496e-05,
      "loss": 1.5907,
      "step": 950
    },
    {
      "epoch": 0.521314146076568,
      "grad_norm": 7.239635467529297,
      "learning_rate": 3.3109619686800894e-05,
      "loss": 1.5935,
      "step": 960
    },
    {
      "epoch": 0.5267445017648656,
      "grad_norm": 4.594213485717773,
      "learning_rate": 3.273676360924683e-05,
      "loss": 1.5981,
      "step": 970
    },
    {
      "epoch": 0.5321748574531632,
      "grad_norm": 4.93820858001709,
      "learning_rate": 3.236390753169277e-05,
      "loss": 1.5575,
      "step": 980
    },
    {
      "epoch": 0.5376052131414608,
      "grad_norm": 4.1672587394714355,
      "learning_rate": 3.1991051454138706e-05,
      "loss": 1.6639,
      "step": 990
    },
    {
      "epoch": 0.5430355688297583,
      "grad_norm": 3.7159271240234375,
      "learning_rate": 3.161819537658464e-05,
      "loss": 1.5375,
      "step": 1000
    },
    {
      "epoch": 0.548465924518056,
      "grad_norm": 5.1484904289245605,
      "learning_rate": 3.124533929903057e-05,
      "loss": 1.6121,
      "step": 1010
    },
    {
      "epoch": 0.5538962802063535,
      "grad_norm": 4.686011791229248,
      "learning_rate": 3.087248322147651e-05,
      "loss": 1.5872,
      "step": 1020
    },
    {
      "epoch": 0.5593266358946511,
      "grad_norm": 5.814095973968506,
      "learning_rate": 3.049962714392245e-05,
      "loss": 1.54,
      "step": 1030
    },
    {
      "epoch": 0.5647569915829487,
      "grad_norm": 5.586899757385254,
      "learning_rate": 3.012677106636838e-05,
      "loss": 1.6129,
      "step": 1040
    },
    {
      "epoch": 0.5701873472712463,
      "grad_norm": 5.848362922668457,
      "learning_rate": 2.9753914988814318e-05,
      "loss": 1.7221,
      "step": 1050
    },
    {
      "epoch": 0.5756177029595438,
      "grad_norm": 5.9575347900390625,
      "learning_rate": 2.9381058911260258e-05,
      "loss": 1.5412,
      "step": 1060
    },
    {
      "epoch": 0.5810480586478415,
      "grad_norm": 4.267794609069824,
      "learning_rate": 2.9008202833706188e-05,
      "loss": 1.5691,
      "step": 1070
    },
    {
      "epoch": 0.586478414336139,
      "grad_norm": 3.777646780014038,
      "learning_rate": 2.8635346756152125e-05,
      "loss": 1.4684,
      "step": 1080
    },
    {
      "epoch": 0.5919087700244366,
      "grad_norm": 5.86043643951416,
      "learning_rate": 2.8262490678598062e-05,
      "loss": 1.7104,
      "step": 1090
    },
    {
      "epoch": 0.5973391257127342,
      "grad_norm": 4.147078037261963,
      "learning_rate": 2.7889634601043996e-05,
      "loss": 1.5492,
      "step": 1100
    },
    {
      "epoch": 0.6027694814010318,
      "grad_norm": 5.54721736907959,
      "learning_rate": 2.7516778523489933e-05,
      "loss": 1.5882,
      "step": 1110
    },
    {
      "epoch": 0.6081998370893293,
      "grad_norm": 5.337580680847168,
      "learning_rate": 2.714392244593587e-05,
      "loss": 1.7145,
      "step": 1120
    },
    {
      "epoch": 0.613630192777627,
      "grad_norm": 4.951231956481934,
      "learning_rate": 2.6771066368381807e-05,
      "loss": 1.6424,
      "step": 1130
    },
    {
      "epoch": 0.6190605484659245,
      "grad_norm": 5.403914451599121,
      "learning_rate": 2.639821029082774e-05,
      "loss": 1.6796,
      "step": 1140
    },
    {
      "epoch": 0.6244909041542221,
      "grad_norm": 9.945127487182617,
      "learning_rate": 2.6025354213273678e-05,
      "loss": 1.6962,
      "step": 1150
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 8.689538955688477,
      "learning_rate": 2.5652498135719615e-05,
      "loss": 1.4885,
      "step": 1160
    },
    {
      "epoch": 0.6353516155308173,
      "grad_norm": 4.529877185821533,
      "learning_rate": 2.527964205816555e-05,
      "loss": 1.5328,
      "step": 1170
    },
    {
      "epoch": 0.6407819712191148,
      "grad_norm": 4.714626312255859,
      "learning_rate": 2.4906785980611485e-05,
      "loss": 1.4445,
      "step": 1180
    },
    {
      "epoch": 0.6462123269074125,
      "grad_norm": 12.565905570983887,
      "learning_rate": 2.453392990305742e-05,
      "loss": 1.6208,
      "step": 1190
    },
    {
      "epoch": 0.65164268259571,
      "grad_norm": 4.680691242218018,
      "learning_rate": 2.416107382550336e-05,
      "loss": 1.653,
      "step": 1200
    },
    {
      "epoch": 0.6570730382840076,
      "grad_norm": 12.357325553894043,
      "learning_rate": 2.3788217747949293e-05,
      "loss": 1.5493,
      "step": 1210
    },
    {
      "epoch": 0.6625033939723052,
      "grad_norm": 6.4424285888671875,
      "learning_rate": 2.3415361670395227e-05,
      "loss": 1.5371,
      "step": 1220
    },
    {
      "epoch": 0.6679337496606028,
      "grad_norm": 4.233353614807129,
      "learning_rate": 2.3042505592841164e-05,
      "loss": 1.7431,
      "step": 1230
    },
    {
      "epoch": 0.6733641053489003,
      "grad_norm": 4.802196979522705,
      "learning_rate": 2.26696495152871e-05,
      "loss": 1.4746,
      "step": 1240
    },
    {
      "epoch": 0.678794461037198,
      "grad_norm": 5.55204439163208,
      "learning_rate": 2.2296793437733035e-05,
      "loss": 1.6292,
      "step": 1250
    },
    {
      "epoch": 0.6842248167254955,
      "grad_norm": 4.072035789489746,
      "learning_rate": 2.192393736017897e-05,
      "loss": 1.5711,
      "step": 1260
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 8.426291465759277,
      "learning_rate": 2.155108128262491e-05,
      "loss": 1.6008,
      "step": 1270
    },
    {
      "epoch": 0.6950855281020907,
      "grad_norm": 7.667903900146484,
      "learning_rate": 2.1178225205070842e-05,
      "loss": 1.5558,
      "step": 1280
    },
    {
      "epoch": 0.7005158837903883,
      "grad_norm": 4.254549980163574,
      "learning_rate": 2.080536912751678e-05,
      "loss": 1.506,
      "step": 1290
    },
    {
      "epoch": 0.7059462394786858,
      "grad_norm": 5.220835208892822,
      "learning_rate": 2.0432513049962716e-05,
      "loss": 1.4936,
      "step": 1300
    },
    {
      "epoch": 0.7113765951669835,
      "grad_norm": 9.15938663482666,
      "learning_rate": 2.0059656972408653e-05,
      "loss": 1.4964,
      "step": 1310
    },
    {
      "epoch": 0.716806950855281,
      "grad_norm": 4.743919372558594,
      "learning_rate": 1.9686800894854587e-05,
      "loss": 1.5064,
      "step": 1320
    },
    {
      "epoch": 0.7222373065435786,
      "grad_norm": 4.434410095214844,
      "learning_rate": 1.931394481730052e-05,
      "loss": 1.4575,
      "step": 1330
    },
    {
      "epoch": 0.7276676622318762,
      "grad_norm": 4.829130172729492,
      "learning_rate": 1.894108873974646e-05,
      "loss": 1.6477,
      "step": 1340
    },
    {
      "epoch": 0.7330980179201738,
      "grad_norm": 4.88580322265625,
      "learning_rate": 1.8568232662192395e-05,
      "loss": 1.4923,
      "step": 1350
    },
    {
      "epoch": 0.7385283736084713,
      "grad_norm": 3.936540126800537,
      "learning_rate": 1.819537658463833e-05,
      "loss": 1.6063,
      "step": 1360
    },
    {
      "epoch": 0.743958729296769,
      "grad_norm": 4.0681586265563965,
      "learning_rate": 1.782252050708427e-05,
      "loss": 1.5173,
      "step": 1370
    },
    {
      "epoch": 0.7493890849850665,
      "grad_norm": 4.713498592376709,
      "learning_rate": 1.7449664429530202e-05,
      "loss": 1.53,
      "step": 1380
    },
    {
      "epoch": 0.7548194406733642,
      "grad_norm": 9.242445945739746,
      "learning_rate": 1.7076808351976136e-05,
      "loss": 1.5856,
      "step": 1390
    },
    {
      "epoch": 0.7602497963616617,
      "grad_norm": 5.975892066955566,
      "learning_rate": 1.6703952274422073e-05,
      "loss": 1.6115,
      "step": 1400
    },
    {
      "epoch": 0.7656801520499593,
      "grad_norm": 9.337746620178223,
      "learning_rate": 1.633109619686801e-05,
      "loss": 1.5368,
      "step": 1410
    },
    {
      "epoch": 0.7711105077382568,
      "grad_norm": 6.361143589019775,
      "learning_rate": 1.5958240119313947e-05,
      "loss": 1.5673,
      "step": 1420
    },
    {
      "epoch": 0.7765408634265545,
      "grad_norm": 4.394771099090576,
      "learning_rate": 1.558538404175988e-05,
      "loss": 1.519,
      "step": 1430
    },
    {
      "epoch": 0.781971219114852,
      "grad_norm": 3.707962989807129,
      "learning_rate": 1.5212527964205816e-05,
      "loss": 1.5401,
      "step": 1440
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 10.36038875579834,
      "learning_rate": 1.4839671886651755e-05,
      "loss": 1.6003,
      "step": 1450
    },
    {
      "epoch": 0.7928319304914472,
      "grad_norm": 4.4281721115112305,
      "learning_rate": 1.4466815809097689e-05,
      "loss": 1.4326,
      "step": 1460
    },
    {
      "epoch": 0.7982622861797448,
      "grad_norm": 3.9679083824157715,
      "learning_rate": 1.4093959731543624e-05,
      "loss": 1.5804,
      "step": 1470
    },
    {
      "epoch": 0.8036926418680423,
      "grad_norm": 6.857390880584717,
      "learning_rate": 1.3721103653989561e-05,
      "loss": 1.6755,
      "step": 1480
    },
    {
      "epoch": 0.80912299755634,
      "grad_norm": 4.546813011169434,
      "learning_rate": 1.3348247576435496e-05,
      "loss": 1.5842,
      "step": 1490
    },
    {
      "epoch": 0.8145533532446375,
      "grad_norm": 7.295271873474121,
      "learning_rate": 1.2975391498881432e-05,
      "loss": 1.5079,
      "step": 1500
    },
    {
      "epoch": 0.8199837089329352,
      "grad_norm": 4.076503753662109,
      "learning_rate": 1.2602535421327369e-05,
      "loss": 1.6037,
      "step": 1510
    },
    {
      "epoch": 0.8254140646212327,
      "grad_norm": 5.47075891494751,
      "learning_rate": 1.2229679343773304e-05,
      "loss": 1.5039,
      "step": 1520
    },
    {
      "epoch": 0.8308444203095303,
      "grad_norm": 3.8829195499420166,
      "learning_rate": 1.185682326621924e-05,
      "loss": 1.4648,
      "step": 1530
    },
    {
      "epoch": 0.8362747759978278,
      "grad_norm": 5.995327949523926,
      "learning_rate": 1.1483967188665176e-05,
      "loss": 1.6184,
      "step": 1540
    },
    {
      "epoch": 0.8417051316861255,
      "grad_norm": 4.220364570617676,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 1.5713,
      "step": 1550
    },
    {
      "epoch": 0.847135487374423,
      "grad_norm": 6.669444561004639,
      "learning_rate": 1.0738255033557047e-05,
      "loss": 1.4767,
      "step": 1560
    },
    {
      "epoch": 0.8525658430627207,
      "grad_norm": 9.881142616271973,
      "learning_rate": 1.0365398956002984e-05,
      "loss": 1.4772,
      "step": 1570
    },
    {
      "epoch": 0.8579961987510182,
      "grad_norm": 5.5376129150390625,
      "learning_rate": 9.99254287844892e-06,
      "loss": 1.5588,
      "step": 1580
    },
    {
      "epoch": 0.8634265544393158,
      "grad_norm": 6.004312038421631,
      "learning_rate": 9.619686800894855e-06,
      "loss": 1.664,
      "step": 1590
    },
    {
      "epoch": 0.8688569101276133,
      "grad_norm": 5.208966255187988,
      "learning_rate": 9.24683072334079e-06,
      "loss": 1.5691,
      "step": 1600
    },
    {
      "epoch": 0.874287265815911,
      "grad_norm": 4.559814453125,
      "learning_rate": 8.873974645786727e-06,
      "loss": 1.5318,
      "step": 1610
    },
    {
      "epoch": 0.8797176215042085,
      "grad_norm": 4.233356952667236,
      "learning_rate": 8.501118568232664e-06,
      "loss": 1.5089,
      "step": 1620
    },
    {
      "epoch": 0.8851479771925062,
      "grad_norm": 5.797980308532715,
      "learning_rate": 8.128262490678598e-06,
      "loss": 1.5213,
      "step": 1630
    },
    {
      "epoch": 0.8905783328808037,
      "grad_norm": 8.388924598693848,
      "learning_rate": 7.755406413124535e-06,
      "loss": 1.6428,
      "step": 1640
    },
    {
      "epoch": 0.8960086885691013,
      "grad_norm": 8.954527854919434,
      "learning_rate": 7.382550335570471e-06,
      "loss": 1.5937,
      "step": 1650
    },
    {
      "epoch": 0.9014390442573988,
      "grad_norm": 5.076608657836914,
      "learning_rate": 7.0096942580164055e-06,
      "loss": 1.6102,
      "step": 1660
    },
    {
      "epoch": 0.9068693999456965,
      "grad_norm": 6.073936939239502,
      "learning_rate": 6.636838180462342e-06,
      "loss": 1.4989,
      "step": 1670
    },
    {
      "epoch": 0.912299755633994,
      "grad_norm": 4.6768598556518555,
      "learning_rate": 6.263982102908278e-06,
      "loss": 1.4901,
      "step": 1680
    },
    {
      "epoch": 0.9177301113222917,
      "grad_norm": 5.164730072021484,
      "learning_rate": 5.891126025354213e-06,
      "loss": 1.6276,
      "step": 1690
    },
    {
      "epoch": 0.9231604670105892,
      "grad_norm": 127.69711303710938,
      "learning_rate": 5.518269947800149e-06,
      "loss": 1.4121,
      "step": 1700
    },
    {
      "epoch": 0.9285908226988868,
      "grad_norm": 5.068517208099365,
      "learning_rate": 5.1454138702460855e-06,
      "loss": 1.5863,
      "step": 1710
    },
    {
      "epoch": 0.9340211783871843,
      "grad_norm": 5.793650150299072,
      "learning_rate": 4.772557792692021e-06,
      "loss": 1.5132,
      "step": 1720
    },
    {
      "epoch": 0.939451534075482,
      "grad_norm": 4.308136940002441,
      "learning_rate": 4.399701715137957e-06,
      "loss": 1.6272,
      "step": 1730
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 4.329166412353516,
      "learning_rate": 4.026845637583892e-06,
      "loss": 1.5183,
      "step": 1740
    },
    {
      "epoch": 0.9503122454520772,
      "grad_norm": 4.085508346557617,
      "learning_rate": 3.6539895600298286e-06,
      "loss": 1.4437,
      "step": 1750
    },
    {
      "epoch": 0.9557426011403747,
      "grad_norm": 4.550946235656738,
      "learning_rate": 3.2811334824757644e-06,
      "loss": 1.557,
      "step": 1760
    },
    {
      "epoch": 0.9611729568286723,
      "grad_norm": 6.257537841796875,
      "learning_rate": 2.9082774049217e-06,
      "loss": 1.6307,
      "step": 1770
    },
    {
      "epoch": 0.9666033125169698,
      "grad_norm": 3.795574903488159,
      "learning_rate": 2.5354213273676363e-06,
      "loss": 1.4336,
      "step": 1780
    },
    {
      "epoch": 0.9720336682052675,
      "grad_norm": 4.746265411376953,
      "learning_rate": 2.162565249813572e-06,
      "loss": 1.5392,
      "step": 1790
    },
    {
      "epoch": 0.977464023893565,
      "grad_norm": 5.768095016479492,
      "learning_rate": 1.7897091722595078e-06,
      "loss": 1.5259,
      "step": 1800
    },
    {
      "epoch": 0.9828943795818627,
      "grad_norm": 8.190932273864746,
      "learning_rate": 1.4168530947054438e-06,
      "loss": 1.5111,
      "step": 1810
    },
    {
      "epoch": 0.9883247352701602,
      "grad_norm": 5.971729278564453,
      "learning_rate": 1.0439970171513796e-06,
      "loss": 1.4831,
      "step": 1820
    },
    {
      "epoch": 0.9937550909584578,
      "grad_norm": 4.374016761779785,
      "learning_rate": 6.711409395973154e-07,
      "loss": 1.5945,
      "step": 1830
    },
    {
      "epoch": 0.9991854466467553,
      "grad_norm": 5.322511672973633,
      "learning_rate": 2.982848620432513e-07,
      "loss": 1.5342,
      "step": 1840
    }
  ],
  "logging_steps": 10,
  "max_steps": 1841,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 1000000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5530254297268224.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
